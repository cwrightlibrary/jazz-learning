<!DOCTYPE html>
<html>
<head>
<title>tutorial.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="want-to-generate-your-own-music-using-deep-learning-heres-a-guide-to-do-just-that">Want to Generate your own Music using Deep Learning? Here’s a Guide to do just that!</h1>
<p>Aravindpai Pai</p>
<p>Updated On January 4th, 2021</p>
<h2 id="overview">Overview</h2>
<ul>
<li>Learn how to develop an end-to-end model for Automatic Music Generation</li>
<li>Understand the WaveNet architecture and implement it from scratch using Keras</li>
<li>Compare the performance of WaveNet versus Long Short Term Memory for building an Automatic Music Generation model</li>
</ul>
<h2 id="introduction">Introduction</h2>
<blockquote>
<p>“If I were not a physicist, I would probably be a musician. I often think in music. I live my daydreams in music. I see my life in terms of music.”
<em>- Albert Einstein</em></p>
</blockquote>
<p>I might not be a physicist like Mr. Einstein, but I wholeheartedly agree with his thoughts on music! I can’t remember a single day when I didn’t open up my music player. My travel to and from office is accompanied by the tune of music and honestly, it helps me focus on my work.</p>
<p>I’ve always dreamed of composing music but didn’t quite get the hang of instruments. That was until I came across <a href="https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning-version2?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">deep learning</a>. Using certain techniques and frameworks, I was able to compose my own original music score without really knowing any music theory!</p>
<p>This was one of my favorite professional projects. I combined my two passions – music and deep learning – to create an automatic music generation model. It’s a dream come true!</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/01/auto-music-.jpg" alt="Robot playing piano"></p>
<p>I am thrilled to share my approach with you, including the entire code to enable you to generate your own music! We’ll first quickly understand the concept of automatic music generation before diving into the different approaches we can use to perform this. Finally, we will fire up Python and design our own automatic music generation model.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#want-to-generate-your-own-music-using-deep-learning-heres-a-guide-to-do-just-that">Want to Generate your own Music using Deep Learning? Here’s a Guide to do just that!</a>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#what-is-automatic-music-generation">What is Automatic Music Generation?</a></li>
<li><a href="#what-are-the-constituent-elements-of-music">What are the Constituent Elements of Music?</a></li>
<li><a href="#different-approaches-to-automatic-music-generation">Different Approaches to Automatic Music Generation</a>
<ul>
<li><a href="#approach-1-using-wavenet">Approach 1: Using WaveNet</a></li>
<li><a href="#approach-2-using-long-short-term-memory-lstm-model">Approach 2: Using Long Short Term Memory (LSTM) Model</a></li>
<li><a href="#wavenet-the-training-phase">Wavenet: The Training Phase</a>
<ul>
<li><a href="#input-to-the-wavenet">Input to the WaveNet:</a></li>
<li><a href="#output-of-the-wavenet">Output of the WaveNet:</a></li>
</ul>
</li>
<li><a href="#inference-phase">Inference phase</a></li>
<li><a href="#understanding-the-wavenet-architecture">Understanding the WaveNet Architecture</a>
<ul>
<li><a href="#why-and-what-is-a-convolution">Why and What is a Convolution?</a></li>
<li><a href="#what-is-1d-convolution">What is 1D Convolution?</a></li>
<li><a href="#pros-of-1d-convolution">Pros of 1D Convolution:</a></li>
<li><a href="#cons-of-1d-convolution">Cons of 1D Convolution:</a></li>
<li><a href="#what-is-1d-causal-convolution">What is 1D Causal Convolution?</a></li>
<li><a href="#pros-of-causal-1d-convolution">Pros of Causal 1D convolution:</a></li>
<li><a href="#cons-of-causal-1d-convolution">Cons of Causal 1D convolution:</a></li>
<li><a href="#what-is-dilated-1d-causal-convolution">What is Dilated 1D Causal Convolution?</a></li>
<li><a href="#pros-of-dilated-1d-causal-convolution">Pros of Dilated 1D Causal Convolution:</a></li>
<li><a href="#residual-block-of-wavenet">Residual Block of WaveNet:</a></li>
<li><a href="#the-workflow-of-wavenet">The Workflow of WaveNet:</a></li>
</ul>
</li>
<li><a href="#long-short-term-memory-lstm-approach">Long Short Term Memory (LSTM) Approach</a>
<ul>
<li><a href="#pros-of-lstm">Pros of LSTM:</a></li>
<li><a href="#cons-of-lstm">Cons of LSTM:</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#implementation--automatic-music-generation-using-python">Implementation – Automatic Music Generation using Python</a>
<ul>
<li><a href="#download-the-dataset">Download the Dataset:</a></li>
<li><a href="#import-libraries">Import libraries:</a></li>
<li><a href="#reading-musical-files">Reading Musical Files:</a></li>
<li><a href="#understanding-the-data">Understanding the data:</a></li>
<li><a href="#preparing-data">Preparing Data:</a></li>
<li><a href="#model-building">Model Building</a></li>
</ul>
</li>
<li><a href="#end-notes">End Notes</a></li>
</ul>
</li>
</ul>
<h2 id="what-is-automatic-music-generation">What is Automatic Music Generation?</h2>
<blockquote>
<p>Music is an Art and a Universal language.</p>
</blockquote>
<p>I define music as a collection of tones of different frequencies. So, the Automatic Music Generation is a process of composing a short piece of music with minimum human intervention.</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/giphy-1.gif" alt="automatic music generation"></p>
<p>What could be the simplest form of generating music?</p>
<p>It all started by randomly selecting sounds and combining them to form a piece of music. In 1787, Mozart proposed a Dice Game for these random sound selections. He composed nearly 272 tones manually! Then, he selected a tone based on the sum of 2 dice.</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/dice_waltz-300x188.gif" alt="automatic music generation"></p>
<p>Another interesting idea was to make use of musical grammar to generate music.</p>
<blockquote>
<p>Musical Grammar comprehends the knowledge necessary to the just arrangement and combination of musical sounds and to the proper performance of musical compositions. <em>- Foundations of Musical Grammar</em></p>
</blockquote>
<p>In the early 1950s, Iannis Xenakis used the concepts of <a href="https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">Statistics and Probability</a> to compose music – popularly known as <strong>Stochastic Music</strong>. He defined music as a sequence of elements (or sounds) that occurs by chance. Hence, he formulated it using stochastic theory. His random selection of elements was strictly dependent on mathematical concepts.</p>
<p>Recently, Deep Learning architectures have become the state of the art for Automatic Music Generation. In this article, I will discuss two different approaches for Automatic Music Composition using WaveNet and LSTM (Long Short Term Memory) architectures.</p>
<p><em>Note: This article requires a basic understanding of a few deep learning concepts. I recommend going through the below articles:</em></p>
<ul>
<li><a href="https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">A Comprehensive Tutorial to learn Convolutional Neural Networks (CNNs) from Scratch</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">Essentials of Deep Learning: Introduction to Long Short Term Memory (LSTM)</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2019/01/sequence-models-deeplearning/?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">Must-Read Tutorial to Learn Sequence Modeling</a></li>
</ul>
<h2 id="what-are-the-constituent-elements-of-music">What are the Constituent Elements of Music?</h2>
<p>Music is essentially composed of Notes and Chords. Let me explain these terms from the perspective of the piano instrument:</p>
<ul>
<li><strong>Note</strong>: The sound produced by a single key is called a note</li>
<li><strong>Chords</strong>: The sound produced by 2 or more keys simultaneously is called a chord. Generally, most chords contain at least 3 key sounds</li>
<li><strong>Octave</strong>: A repeated pattern is called an octave. Each octave contains 7 white and 5 black keys</li>
</ul>
<h2 id="different-approaches-to-automatic-music-generation">Different Approaches to Automatic Music Generation</h2>
<p>I will discuss two Deep Learning-based architectures in detail for automatically generating music – WaveNet and LSTM. But, why only Deep Learning architectures?</p>
<p>Deep Learning is a field of Machine Learning which is inspired by a neural structure. These networks extract the features automatically from the dataset and are capable of learning any non-linear function. That’s why Neural Networks are called as <strong>Universal Functional Approximators</strong>.</p>
<p>Hence, Deep Learning models are the state of the art in various fields like <a href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">Natural Language Processing (NLP)</a>, <a href="https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning-version2?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">Computer Vision</a>, Speech Synthesis and so on. Let’s see how we can build these models for music composition.</p>
<h3 id="approach-1-using-wavenet">Approach 1: Using WaveNet</h3>
<blockquote>
<p>WaveNet is a Deep Learning-based generative model for raw audio developed by Google DeepMind.</p>
</blockquote>
<p>The main objective of WaveNet is to generate new samples from the original distribution of the data. Hence, it is known as a Generative Model.</p>
<blockquote>
<p>Wavenet is like a language model from NLP.</p>
</blockquote>
<p>In a <a href="https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">language model</a>, given a sequence of words, the model tries to predict the next word. Similar to a language model, in WaveNet, given a sequence of samples, it tries to predict the next sample.</p>
<h3 id="approach-2-using-long-short-term-memory-lstm-model">Approach 2: Using Long Short Term Memory (LSTM) Model</h3>
<p><a href="https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">Long Short Term Memory Model</a>, popularly known as LSTM, is a variant of <a href="https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">Recurrent Neural Networks (RNNs)</a> that is capable of capturing the long term dependencies in the input sequence. LSTM has a wide range of applications in Sequence-to-Sequence modeling tasks like Speech Recognition, Text Summarization, Video Classification, and so on.</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/lstm.png" alt="lstm"></p>
<p>Let’s discuss in detail how we can train our model using these two approaches.</p>
<h3 id="wavenet-the-training-phase">Wavenet: The Training Phase</h3>
<p>This is a Many-to-One problem where the input is a sequence of amplitude values and the output is the subsequent value.</p>
<p>Let’s see how we can prepare input and output sequences.</p>
<h4 id="input-to-the-wavenet">Input to the WaveNet:</h4>
<p>WaveNet takes the chunk of a raw audio wave as an input. Raw audio wave refers to the representation of a wave in the time series domain.</p>
<p>In the time-series domain, an audio wave is represented in the form of amplitude values which are recorded at different intervals of time:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/unnamed.gif" alt="wave"></p>
<h4 id="output-of-the-wavenet">Output of the WaveNet:</h4>
<p>Given the sequence of the amplitude values, WaveNet tries to predict the successive amplitude value.</p>
<p>Let’s understand this with the help of an example. Consider an audio wave of 5 seconds with a sampling rate of 16,000 (that is 16,000 samples per second). Now, we have 80,000 samples recorded at different intervals for 5 seconds. Let’s break the audio into chunks of equal size, say 1024 (which is a hyperparameter).</p>
<p>The below diagram illustrates the input and output sequences for the model:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/chunk.png" alt="chunk"></p>
<p>Input and Output of first 3 chunks</p>
<p>We can follow a similar procedure for the rest of the chunks.</p>
<p>We can infer from the above that the output of every chunk depends only on the past information ( i.e. previous timesteps) but not on the future timesteps. Hence, this task is known as <strong>Autoregressive task</strong> and the model is known as an <strong>Autoregressive model</strong>.</p>
<h3 id="inference-phase">Inference phase</h3>
<p>In the inference phase, we will try to generate new samples. Let’s see how to do that:</p>
<ol>
<li>Select a random array of sample values as a starting point to model</li>
<li>Now, the model outputs the probability distribution over all the samples</li>
<li>Choose the value with the maximum probability and append it to an array of samples</li>
<li>Delete the first element and pass as an input for the next iteration</li>
<li>Repeat steps 2 and 4 for a certain number of iterations</li>
</ol>
<h3 id="understanding-the-wavenet-architecture">Understanding the WaveNet Architecture</h3>
<p>The building blocks of WaveNet are <strong>Causal Dilated 1D Convolution layers</strong>. Let us first understand the importance of the related concepts.</p>
<h4 id="why-and-what-is-a-convolution">Why and What is a Convolution?</h4>
<blockquote>
<p>One of the main reasons for using convolution is to extract the features from an input.</p>
</blockquote>
<p>For example, in the case of image processing, convolving the image with a filter gives us a feature map.</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/conv_output.jpg" alt="music convolving"></p>
<p><strong>Convolution is a mathematical operation that combines 2 functions</strong>. In the case of image processing, convolution is a linear combination of certain parts of an image with the kernel.</p>
<p>You can browse through the below article to read more about convolution:</p>
<ul>
<li><a href="https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/?utm_source=blog&amp;utm_medium=how-to-perform-automatic-music-generation">Architecture of Convolutional Neural Networks (CNNs) Demystified</a></li>
</ul>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/f2RiP.gif" alt="convolution"></p>
<h4 id="what-is-1d-convolution">What is 1D Convolution?</h4>
<p>The objective of 1D convolution is similar to the Long Short Term Memory model. It is used to solve similar tasks to those of LSTM. In 1D convolution, a kernel or a filter moves along only one direction:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/Calculations-involved-in-a-1D-convolution-operation-300x212.png" alt="convolution"></p>
<p>The output of convolution depends upon the size of the kernel, input shape, type of padding, and stride. Now, I will walk you through different types of padding for understanding the importance of using Dilated Causal 1D Convolution layers.</p>
<p>When we set the padding <strong>valid</strong>, the input and output sequences vary in length. The length of an output is less than an input:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/conv-valid.jpg" alt="padding"></p>
<p>When we set the padding to <strong>same</strong>, zeroes are padded on either side of the input sequence to make the length of input and output equal:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/padding_same.jpg" alt="conv"></p>
<h4 id="pros-of-1d-convolution">Pros of 1D Convolution:</h4>
<ul>
<li>Captures the sequential information present in the input sequence</li>
<li>Training is much faster compared to GRU or LSTM because of the absence of recurrent connections</li>
</ul>
<h4 id="cons-of-1d-convolution">Cons of 1D Convolution:</h4>
<ul>
<li>When padding is set to the <strong>same</strong>, output at timestep <strong>t</strong> is convolved with the previous <strong>t-1</strong> and future timesteps <strong>t+1</strong> too. Hence, it violates the Autoregressive principle</li>
<li>When padding is set to <strong>valid</strong>, input and output sequences vary in length which is required for computing residual connections (which will be covered later)</li>
</ul>
<p>This clears the way for the Causal Convolution.</p>
<p><strong>Note:</strong> <em>The pros and Cons I mentioned here are specific to this problem.</em></p>
<h4 id="what-is-1d-causal-convolution">What is 1D Causal Convolution?</h4>
<blockquote>
<p>This is defined as convolutions where output at time <strong>t</strong> is convolved only with elements from time <strong>t</strong> and earlier in the previous layer.</p>
</blockquote>
<p>In simpler terms, normal and causal convolutions differ only in padding. In causal convolution, zeroes are added to the left of the input sequence to preserve the principle of autoregressive:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/casua1-1.jpg" alt="ca"></p>
<h4 id="pros-of-causal-1d-convolution">Pros of Causal 1D convolution:</h4>
<ul>
<li>Causal convolution does not take into account the future timesteps which is a criterion for building a Generative model</li>
</ul>
<h4 id="cons-of-causal-1d-convolution">Cons of Causal 1D convolution:</h4>
<ul>
<li>Causal convolution cannot look back into the past or the timesteps that occurred earlier in the sequence. Hence, causal convolution has a very low receptive field. The receptive field of a network refers to the number of inputs influencing an output:</li>
</ul>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/cas.jpg" alt="causal"></p>
<p>As you can see here, the output is influenced by only 5 inputs. Hence, the Receptive field of the network is 5, which is very low. The receptive field of a network can also be increased by adding kernels of large sizes but keep in mind that the computational complexity increases.</p>
<p>This drives us to the awesome concept of the Dilated 1D Causal Convolution.</p>
<h4 id="what-is-dilated-1d-causal-convolution">What is Dilated 1D Causal Convolution?</h4>
<blockquote>
<p>A Causal 1D convolution layer with the holes or spaces in between the values of a kernel is known as Dilated 1D convolution.</p>
</blockquote>
<p>The number of spaces to be added is given by the dilation rate. It defines the reception field of a network. A kernel of size <strong>k</strong> and dilation rate <strong>d</strong> has <strong>d-1</strong> holes in between every value in kernel <strong>k</strong>.</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/dilated.jpg" alt="dilated"></p>
<p>As you can see here, convolving a 3 * 3 kernel over a 7 * 7 input with dilation rate 2 has a reception field of 5 * 5.</p>
<h4 id="pros-of-dilated-1d-causal-convolution">Pros of Dilated 1D Causal Convolution:</h4>
<ul>
<li>The dilated 1D convolution network increases the receptive field by exponentially increasing the dilation rate at every hidden layer:</li>
</ul>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/dil.jpg" alt="conv1d"></p>
<p>As you can see here, the output is influenced by all the inputs. Hence, the receptive field of the network is 16.</p>
<h4 id="residual-block-of-wavenet">Residual Block of WaveNet:</h4>
<p>A building block contains Residual and Skip connections which are just added to speed up the convergence of the model:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/arcg.jpg" alt="wavenet"></p>
<h4 id="the-workflow-of-wavenet">The Workflow of WaveNet:</h4>
<ul>
<li>Input is fed into a causal 1D convolution</li>
<li>The output is then fed to 2 different dilated 1D convolution layers with <strong>sigmoid</strong> and <strong>tanh</strong> activations</li>
<li>The element-wise multiplication of 2 different activation values results in a skip connection</li>
<li>And the element-wise addition of a skip connection and output of causal 1D results in the residual</li>
</ul>
<h3 id="long-short-term-memory-lstm-approach">Long Short Term Memory (LSTM) Approach</h3>
<p>Another approach for automatic music generation is based on the Long Short Term Memory (LSTM) model. The preparation of input and output sequences is similar to WaveNet. At each timestep, an amplitude value is fed into the Long Short Term Memory cell – it then computes the hidden vector and passes it on to the next timesteps.</p>
<p>The current hidden vector at timestep <strong>h<sub>t</sub></strong> is computed based on the current input <strong>a<sub>t</sub></strong> and previously hidden vector <strong>h<sub>t-1</sub></strong>. This is how the sequential information is captured in any Recurrent Neural Network:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/lstm.jpg" alt="lstm"></p>
<h4 id="pros-of-lstm">Pros of LSTM:</h4>
<ul>
<li>Captures the sequential information present in the input sequence</li>
</ul>
<h4 id="cons-of-lstm">Cons of LSTM:</h4>
<ul>
<li>It consumes a lot of time for training since it processes the inputs sequentially</li>
</ul>
<h2 id="implementation-%E2%80%93-automatic-music-generation-using-python">Implementation – Automatic Music Generation using Python</h2>
<p>The wait is over! Let’s develop an end-to-end model for the automatic generation of music. Fire up your Jupyter notebooks or Colab (or whichever IDE you prefer).</p>
<h3 id="download-the-dataset">Download the Dataset:</h3>
<p>I downloaded and combined multiple classical music files of a digital piano from numerous resources. You can download the final dataset from <a href="https://drive.google.com/file/d/1qnQVK17DNVkU19MgVA4Vg88zRDvwCRXw/view?usp=sharing">here</a>.</p>
<h3 id="import-libraries">Import libraries:</h3>
<p><strong>Music 21</strong> is a Python library developed by MIT for understanding music data. MIDI is a standard format for storing music files. MIDI stands for <strong>Musical Instrument Digital Interface</strong>. MIDI files contain the instructions rather than the actual audio. Hence, it occupies very little memory. That’s why it is usually preferred while transferring files.</p>
<pre class="hljs"><code><div>#library for understanding music
from music21 import *
</div></code></pre>
<h3 id="reading-musical-files">Reading Musical Files:</h3>
<p>Let’s define a function straight away for reading the MIDI files. It returns the array of notes and chords present in the musical file.</p>
<p><strong>music_1.py</strong></p>
<pre class="hljs"><code><div>#defining function to read MIDI files
def read_midi(file):
    
    print(&quot;Loading Music File:&quot;,file)
    
    notes=[]
    notes_to_parse = None
    
    #parsing a midi file
    midi = converter.parse(file)
  
    #grouping based on different instruments
    s2 = instrument.partitionByInstrument(midi)

    #Looping over all the instruments
    for part in s2.parts:
    
        #select elements of only piano
        if 'Piano' in str(part): 
        
            notes_to_parse = part.recurse() 
      
            #finding whether a particular element is note or a chord
            for element in notes_to_parse:
                
                #note
                if isinstance(element, note.Note):
                    notes.append(str(element.pitch))
                
                #chord
                elif isinstance(element, chord.Chord):
                    notes.append('.'.join(str(n) for n in element.normalOrder))

    return np.array(notes)
</div></code></pre>
<p>Now, we will load the MIDI files into our environment.</p>
<p><strong>music_2.py</strong></p>
<pre class="hljs"><code><div>#for listing down the file names
import os

#Array Processing
import numpy as np

#specify the path
path='schubert/'

#read all the filenames
files=[i for i in os.listdir(path) if i.endswith(&quot;.mid&quot;)]

#reading each midi file
notes_array = np.array([read_midi(path+i) for i in files])
</div></code></pre>
<h3 id="understanding-the-data">Understanding the data:</h3>
<p><strong>music_3.py</strong></p>
<pre class="hljs"><code><div>#converting 2D array into 1D array
notes_ = [element for note_ in notes_array for element in note_]

#No. of unique notes
unique_notes = list(set(notes_))
print(len(unique_notes))
</div></code></pre>
<p><strong>Output</strong>: 304</p>
<p>As you can see here, no. of unique notes is 304. Now, let us see the distribution of the notes.</p>
<p><strong>music_4.py</strong></p>
<pre class="hljs"><code><div>#importing library
from collections import Counter

#computing frequency of each note
freq = dict(Counter(notes_))

#library for visualiation
import matplotlib.pyplot as plt

#consider only the frequencies
no=[count for _,count in freq.items()]

#set the figure size
plt.figure(figsize=(5,5))

#plot
plt.hist(no)
</div></code></pre>
<p><strong>Output</strong>:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/01/download-300x284.png" alt="music generation"></p>
<p>From the above plot, we can infer that most of the notes have a very low frequency. So, let us keep the top frequent notes and ignore the low-frequency ones. Here, I am defining the threshold as 50. Nevertheless, the parameter can be changed.</p>
<pre class="hljs"><code><div>frequent_notes = [note_ for note_, count in freq.items() if count&gt;=50]
print(len(frequent_notes))
</div></code></pre>
<p><strong>Output</strong>: 167</p>
<p>As you can see here, no. of frequently occurring notes is around 170.  Now, let us prepare new musical files which contain only the top frequent notes</p>
<p><strong>music_5.py</strong></p>
<pre class="hljs"><code><div>new_music=[]

for notes in notes_array:
    temp=[]
    for note_ in notes:
        if note_ in frequent_notes:
            temp.append(note_)            
    new_music.append(temp)
    
new_music = np.array(new_music)
</div></code></pre>
<h3 id="preparing-data">Preparing Data:</h3>
<p>Preparing the input and output sequences as mentioned in the article:</p>
<p><strong>music_6.py</strong></p>
<pre class="hljs"><code><div>no_of_timesteps = 32
x = []
y = []

for note_ in new_music:
    for i in range(0, len(note_) - no_of_timesteps, 1):
        
        #preparing input and output sequences
        input_ = note_[i:i + no_of_timesteps]
        output = note_[i + no_of_timesteps]
        
        x.append(input_)
        y.append(output)
        
x=np.array(x)
y=np.array(y)
</div></code></pre>
<p>Now, we will assign a unique integer to every note:</p>
<pre class="hljs"><code><div>unique_x = list(set(x.ravel()))
x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))
</div></code></pre>
<p>We will prepare the integer sequences for input data.</p>
<p><strong>music_7.py</strong></p>
<pre class="hljs"><code><div>#preparing input sequences
x_seq=[]
for i in x:
    temp=[]
    for j in i:
        #assigning unique integer to every note
        temp.append(x_note_to_int[j])
    x_seq.append(temp)
    
x_seq = np.array(x_seq)
</div></code></pre>
<p>Similarly, prepare the integer sequences for output data as well.</p>
<pre class="hljs"><code><div>unique_y = list(set(y))
y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) 
y_seq=np.array([y_note_to_int[i] for i in y])
</div></code></pre>
<p>Let us preserve 80% of the data for training and the rest 20% for the evaluation:</p>
<pre class="hljs"><code><div>from sklearn.model_selection import train_test_split
x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)
</div></code></pre>
<h3 id="model-building">Model Building</h3>
<p>I have defined 2 architectures here – WaveNet and LSTM. Please experiment with both the architectures to understand the importance of WaveNet architecture.</p>
<p><strong>build.py</strong></p>
<pre class="hljs"><code><div>model = simple_wavenet()
model.fit(X,np.array(y), epochs=300, batch_size=128,callbacks=[mc])
</div></code></pre>
<p><strong>callback.py</strong></p>
<pre class="hljs"><code><div>import keras
mc = keras.callbacks.ModelCheckpoint('model{epoch:03d}.h5', save_weights_only=False, period=50)
</div></code></pre>
<p><strong>create_midi.py</strong></p>
<pre class="hljs"><code><div>def convert_to_midi(prediction_output):
    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # Specify duration between 2 notes
        offset+  = 0.5
       # offset += random.uniform(0.5,0.9)

    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp='music.mid')
</div></code></pre>
<p><strong>generate.py</strong></p>
<pre class="hljs"><code><div>#Select random chunk for the first iteration
start = np.random.randint(0, len(X)-1)
pattern = X[start]
#load the best model
model=load_model('model300.h5')
#generate and save music
music = generate_music(model,pitch,no_of_timesteps,pattern)
convert_to_midi(music)
</div></code></pre>
<p><strong>generate_music.py</strong></p>
<pre class="hljs"><code><div>#Select random chunk for the first iteration
start = np.random.randint(0, len(X)-1)
pattern = X[start]
#load the best model
model=load_model('model300.h5')
#generate and save music
music = generate_music(model,pitch,no_of_timesteps,pattern)
convert_to_midi(music)
</div></code></pre>
<p><strong>import.py</strong></p>
<pre class="hljs"><code><div>#dealing with midi files
from music21 import * 

#array processing
import numpy as np     
import os

#random number generator
import random         

#keras for building deep learning model
from keras.layers import * 
from keras.models import *
import keras.backend as K
</div></code></pre>
<p><strong>lstm.py</strong></p>
<pre class="hljs"><code><div>def lstm():
	model = Sequential()
	model.add(LSTM(128,return_sequences=True))
	model.add(LSTM(128))
	model.add(Dense(256))
	model.add(Activation('relu'))
	model.add(Dense(n_vocab))
	model.add(Activation('softmax'))
	model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')
	return model
</div></code></pre>
<p><strong>prepare.py</strong></p>
<pre class="hljs"><code><div>#length of a input sequence
no_of_timesteps = 128      

#no. of unique notes
n_vocab = len(set(notes))  

#all the unique notes
pitch = sorted(set(item for item in notes))  

#assign unique value to every note
note_to_int = dict((note, number) for number, note in enumerate(pitch))  

#preparing input and output sequences
X = []
y = []
for notes in all_notes:
  for i in range(0, len(notes) - no_of_timesteps, 1):
    input_ = notes[i:i + no_of_timesteps]
    output = notes[i + no_of_timesteps]
    X.append([note_to_int[note] for note in input_])
    y.append(note_to_int[output])
</div></code></pre>
<p><strong>read_data.py</strong></p>
<pre class="hljs"><code><div>#read all the filenames
files=[i for i in os.listdir() if i.endswith(&quot;.mid&quot;)]

#reading each midi file
all_notes=[]
for i in files:
  all_notes.append(read_midi(i))

#notes and chords of all the midi files
notes = [element for notes in all_notes for element in notes]
</div></code></pre>
<p><strong>read_midi.py</strong></p>
<pre class="hljs"><code><div>def read_midi(file):
  notes=[]
  notes_to_parse = None

  #parsing a midi file
  midi = converter.parse(file)
  #grouping based on different instruments
  s2 = instrument.partitionByInstrument(midi)

  #Looping over all the instruments
  for part in s2.parts:
    #select elements of only piano
    if 'Piano' in str(part): 
      notes_to_parse = part.recurse() 
      #finding whether a particular element is note or a chord
      for element in notes_to_parse:
        if isinstance(element, note.Note):
          notes.append(str(element.pitch))
        elif isinstance(element, chord.Chord):
          notes.append('.'.join(str(n) for n in element.normalOrder))
      
  return notes
</div></code></pre>
<p><strong>reshaping.py</strong></p>
<pre class="hljs"><code><div>#reshaping
X = np.reshape(X, (len(X), no_of_timesteps, 1))
#normalizing the inputs
X = X / float(n_vocab) 
</div></code></pre>
<p><strong>seed.py</strong></p>
<pre class="hljs"><code><div>from numpy.random import seed
seed(1)
from tensorflow import set_random_seed
set_random_seed(2)
</div></code></pre>
<p><strong>wavenet.py</strong></p>
<pre class="hljs"><code><div>K.clear_session()
def simple_wavenet():
  no_of_kernels=64
  num_of_blocks= int(np.sqrt(no_of_timesteps)) - 1   #no. of stacked conv1d layers

  model = Sequential()
  for i in range(num_of_blocks):
    model.add(Conv1D(no_of_kernels,3,dilation_rate=(2**i),padding='causal',activation='relu'))
  model.add(Conv1D(1, 1, activation='relu', padding='causal'))
  model.add(Flatten())
  model.add(Dense(128, activation='relu'))
  model.add(Dense(n_vocab, activation='softmax'))
  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')
  return model
</div></code></pre>
<p>I have simplified the architecture of the WaveNet without adding residual and skip connections since the role of these layers is to improve the faster convergence (and WaveNet takes raw audio wave as input). But in our case, the input would be a set of nodes and chords since we are generating music:</p>
<p><strong>10_8.py</strong></p>
<pre class="hljs"><code><div>from keras.layers import *
from keras.models import *
from keras.callbacks import *
import keras.backend as K

K.clear_session()
model = Sequential()
    
#embedding layer
model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) 

model.add(Conv1D(64,3, padding='causal',activation='relu'))
model.add(Dropout(0.2))
model.add(MaxPool1D(2))
    
model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))
model.add(Dropout(0.2))
model.add(MaxPool1D(2))

model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))
model.add(Dropout(0.2))
model.add(MaxPool1D(2))
          
#model.add(Conv1D(256,5,activation='relu'))    
model.add(GlobalMaxPool1D())
    
model.add(Dense(256, activation='relu'))
model.add(Dense(len(unique_y), activation='softmax'))
    
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')

model.summary()
</div></code></pre>
<p>Define the callback to save the best model during training:</p>
<pre class="hljs"><code><div>mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)
</div></code></pre>
<p>Let’s train the model with a batch size of 128 for 50 epochs:</p>
<pre class="hljs"><code><div>history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])
</div></code></pre>
<p>Loading the best model:</p>
<pre class="hljs"><code><div>#loading best model
from keras.models import load_model
model = load_model('best_model.h5')
</div></code></pre>
<p>Its time to compose our own music now. We will follow the steps mentioned under the inference phase for the predictions.</p>
<p><strong>10_9.py</strong></p>
<pre class="hljs"><code><div>import random
ind = np.random.randint(0,len(x_val)-1)

random_music = x_val[ind]

predictions=[]
for i in range(10):

    random_music = random_music.reshape(1,no_of_timesteps)

    prob  = model.predict(random_music)[0]
    y_pred= np.argmax(prob,axis=0)
    predictions.append(y_pred)

    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)
    random_music = random_music[1:]
    
print(predictions)
</div></code></pre>
<p>Now, we will convert the integers back into the notes.</p>
<pre class="hljs"><code><div>x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) 
predicted_notes = [x_int_to_note[i] for i in predictions]
</div></code></pre>
<p>The final step is to convert back the predictions into a MIDI file. Let’s define the function to accomplish the task.</p>
<p><strong>10_10.py</strong></p>
<pre class="hljs"><code><div>def convert_to_midi(prediction_output):
   
    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                
                cn=int(current_note)
                new_note = note.Note(cn)
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
                
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
            
        # pattern is a note
        else:
            
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 1
    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp='music.mid')
</div></code></pre>
<p>Converting the predictions into a musical file:</p>
<pre class="hljs"><code><div>convert_to_midi(predicted_notes)
</div></code></pre>
<p>Some of the tunes composed by the model:</p>
<p><a href="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/01/music_1.mp3?_=1">Audio 1</a></p>
<p><a href="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/01/music_2.mp3?_=2">Audio 2</a></p>
<p><a href="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/01/music.mp3?_=3">Audio 3</a></p>
<p>Awesome, right? But your learning doesn’t stop here. Just remember that we have built a <strong>baseline model</strong>. There are plenty of ways to improve the performance of the model even further:</p>
<ul>
<li>As the size of the training dataset is small, we can fine-tune a pre-trained model to build a robust system</li>
<li>Collect as much as training data as you can since the deep learning model generalizes well on the larger datasets</li>
</ul>
<h2 id="end-notes">End Notes</h2>
<p>Deep Learning has a wide range of applications in our daily life. The key steps in solving any problem are understanding the problem statement, formulating it and defining the architecture to solve the problem.</p>
<p>I had a lot of fun (and learning) while working on this project. Music is a passion of mine and it was quite intriguing combining deep learning with that.</p>
<p>I am looking forward to hearing your approach to the problem in the comments section. And if you have any feedback on this article or any doubts/queries, kindly share them in the comments section below and I will get back to you.</p>

</body>
</html>
